<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <title>Encoding: value types to binary</title>
    <link rel="stylesheet" href="../gitdb.css" type="text/css" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="description" content="gitdb - decentralized, replicating transactional store" />
    <meta name="author" content="Sam Vilain" />
    <meta name="copyright" content="&copy; 2009-2011 Sam Vilain" />
    <meta name="keywords" content="gitdb, Git-DB, database, decentralized, transactional, ACID, git, avro, thrift, IDL, Protocol Buffers,
    " />
</head>
<body>
  <div id="sitelogo">
    <a href="../index.html">
      <img id="logo" src="../files/gitdb-logo.png" border="0" />
      <div id="titlebox">
        <span id="site_title">git db</span>
        <span id="site_strapline">decentralized data storage in git</span>
      </div>
    </a>
  </div>

  <div id="navbar">
    <div class="menuItem">
      <a href="../design/index.html" class="menu">Design</a>
    </div>
    <div class="menuItem">
      <a href="../code.html" class="menu">Code</a>
    </div>
    <div class="menuItem">
      <a href="../examples.html" class="menu">Examples</a>
    </div>
  </div><!-- id="navbar" -->

    <ul id="breadcrumbs">
      <li><a href="../index.html">Index</a></li>
<li>&gt;</li>
<li><a href="index.html">Design</a></li>
<li>&gt;</li>
<li>Encoding: value types to binary</li>

    </ul>

  <div id="body">
    <div class="document" id="encoding-value-types-to-binary">
<h1 class="title">Encoding: value types to binary</h1>
<p>This page describes how various elemental value types are encoded.
When decoding, you need to know what type to expect.  In the next
level, <a class="reference external" href="./columnformat.html">ColumnFormat</a>, it is described how these expectations are
encoded.  Much later, after the <a class="reference external" href="./meta.html">MetaFormat</a> is known then it is
possible to interpret the values to a type.</p>
<div class="section" id="encoding-unsigned-integers">
<h1>Encoding Unsigned Integers</h1>
<div class="figure align-right" style="width: 25em">
<img alt="diagram showing binary representation of several unsigned integers" src="../files/varuint.png" style="width: 20em;" />
<p class="caption"><strong>Figure 1.</strong> variable-length unsigned integer encoding; used
by Perl and Google ProtocolBuffer.</p>
</div>
<p>Integers are widely encoded in binary by the design.  For unsigned
integers, a simple variable length integer encoding is used.  This is
essentially the same as that used by Perl's <tt class="docutils literal">pack &quot;w&quot;, $value</tt>
and in Google's ProtocolBuffer.</p>
<p>This is a very straightforward convention.  The top bit of every byte
except the last is set, and the number is contained inside.  The
encoded number is reconstructed by assembling all of the 7-bit
fragments together into a binary number.</p>
<p>There can be differences in whether the bytes appearing first or last
are most significant (&quot;Big Endian&quot;) or least significant (&quot;Little
Endian&quot;).  This standard is Big Endian, but that doesn't really make
much difference to performance on Little Endian systems (which are
most of the computers in the world) as they are not native machine
words anyway.</p>
</div>
<div class="section" id="encoding-signed-integers">
<h1>Encoding Signed Integers</h1>
<p>In various places, it is possible to encode a number which may be
negative.  In JSON, the values are just stored in base 10.  Signed
integers appear in other parts of the Column Format as
variable-length, twos-complement quantities.</p>
<div class="figure align-right" style="width: 25em">
<img alt="diagram showing binary representation of several signed integers" src="../files/varint.png" style="width: 20em;" />
<p class="caption"><strong>Figure 2.</strong> The variable-length signed integer encoding used by
this design.  The sign bit is highlighted in green.</p>
</div>
<p>Perl's built-in function does not permit storing negative numbers, and
in Google's <a class="reference external" href="http://code.google.com/apis/protocolbuffers/docs/encoding.html">ProtocolBuffers Encoding documentation</a>, it is written
(emphasis added):</p>
<blockquote>
The lower 7 bits of each byte are used to store the <em>two's
complement representation</em> of the number in groups of 7 bits, ...</blockquote>
<p>This standard actually implements that, and it is simple,
straightforward and efficient.  The diagram to the right shows how it
works with various values.</p>
<p>However, ProtocolBuffers then does something very weird when it comes
to signed integers:</p>
<blockquote>
If you use <tt class="docutils literal">int32</tt> or <tt class="docutils literal">int64</tt> as the type for a negative
number, the resulting varint is <em>always ten bytes long</em> – it is,
effectively, treated like a very large unsigned integer.  If you
use one of the signed types, the resulting varint uses ZigZag
encoding, which is much more efficient.</blockquote>
<p>I'm left thinking, so if I decode a 7-bit quantity with its high bit
set, am I to interpret that as a negative number, as the &quot;two's
complement representation&quot; written would imply, or as a 7-bit positive
number?</p>
<p>I came to the conclusion that they'd simply stuffed up the spec.  It
wasn't a variable-length 2's complement number at all.  The number is
to be considered negative, <em>if bit 63 in the number is set</em>.  The
standard is effectively married to 64-bit integers, and I really
didn't want that.</p>
<p>In fact they invented a whole new type code to work around this bug,
even though the answer is obvious - simply extend the sign bit in the
decoded number across the entire machine word.  Conveniently, it's
even a single instruction on x86 processors (<tt class="docutils literal">SAR</tt> / <tt class="docutils literal">SAL</tt>).  When
working with signed integers in C, the <tt class="docutils literal">&gt;&gt;</tt> operator refers to this
signed shift.</p>
<p>To &quot;convert&quot; a number <em>N</em>, decoded from <em>X</em> bytes, to a native signed
integer of <em>W</em> bits width, you therefore use:</p>
<blockquote>
<tt class="docutils literal">(int)( (unsigned int)N &lt;&lt; <span class="pre">(W-X*7)</span> ) &gt;&gt; <span class="pre">(W-X*7)</span></tt></blockquote>
<p>More likely, to &quot;convert&quot; a number Nt, decoded from X bytes, the lower
7 bits of each were packed into it from the top down, that's:</p>
<blockquote>
<tt class="docutils literal">Nt &lt;&lt; <span class="pre">(W-X*7)</span></tt></blockquote>
</div>
<div class="section" id="encoding-other-numeric-quantities">
<h1>Encoding other Numeric quantities</h1>
<p>There are several second-order numeric types which are encoded.  These
are all built on the integer formats.  Currently they all involve
storing two variable length, signed integers.</p>
<table border="1" class="docutils">
<colgroup>
<col width="14%" />
<col width="14%" />
<col width="43%" />
<col width="29%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Name</th>
<th class="head">Formula</th>
<th class="head">Description</th>
<th class="head">JSON</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>float</td>
<td><em>N</em> × 2<sup>*M*</sup></td>
<td>Floating point: general purpose, &quot;real&quot; numbers</td>
<td>(none)</td>
</tr>
<tr><td>decimal</td>
<td><em>N</em> × 10<sup>*M*</sup></td>
<td>Decimal precision values (eg money or IEEE decimal formats)</td>
<td><tt class="docutils literal">100.1</tt>, <tt class="docutils literal">1.2345e+5</tt></td>
</tr>
<tr><td>rational</td>
<td>N ÷ M</td>
<td>Arbitrary precision rational numbers</td>
<td><tt class="docutils literal">4/37</tt>, <tt class="docutils literal">12345/23941</tt></td>
</tr>
</tbody>
</table>
<p>Converting from the two integers N and M to a corresponding floating
point value is trivial in each case; though see the below section for
how &quot;special&quot; IEEE floating point values are encoded.</p>
<p>In JSON data form, there is no 'float', but instead 'decimal' is used.
This is because in JSON you cannot have literals in hexadecimal, such
as &quot;0x1.ef0fp+0&quot;, as in C, Python, etc.  It's possible to encode a
true rational as a JavaScript expression, and you can express all
floating point numbers precisely as a rational, but this is not
recommended for general applications.  Few if any JSON parsers will
read it without exception anyway.</p>
<p>Architecture-dependent floats can be easily transformed into the
underlying integers, through either directly accessing the bit fields
of the floating point word (eg, from C), or using a relatively IEEE
math (from other languages).</p>
<p>Only implementations and applications that care about precision of the
values they are storing need implement output for <tt class="docutils literal">decimal</tt> and
<tt class="docutils literal">rational</tt>; others need only know how to read them.  It is also
possible to specify in the <a class="reference external" href="./meta.html">MetaFormat</a> that only particular encodings
are allowed for a given type.</p>
</div>
<div class="section" id="floating-point-representation">
<h1>Floating Point Representation</h1>
<p>Again, it seems like the standard has ended up re-inventing the wheel
by coming up with its own floating point representation.  However, the
need for this arose from actually trying to implement the output
format.</p>
<p>Specifying a particular common format, such as IEEE 754-2008 binary64
(as ProtocolBuffers uses) was certainly the first choice.  However,
there are problems if the native float format is not binary64; this
might seem unlikely, but it happens.  And emitting 64-bit floats if
that is not what your hardware is using can be problematic, too.
Finally, if you are working with for instance 80-bit floats and the
storage system loses 11 bits of precision when you put values in and
out, this can be an annoyance.</p>
<p>The IEEE 754 standard is a good choice for a machine word, and makes
computations fast and squeezes every bit of precision it can out of
the available space.  However the requirements of this standard are
slightly different.  It doesn't matter a lot if the stored size is
slightly larger than the machine format, just so long as all platforms
can read and write them.</p>
<div class="figure align-right" style="width: 25em">
<img alt="diagram showing floating point unpacking of various single precision floating point numbers" src="../files/floats.png" style="width: 20em;" />
<p class="caption"><strong>Figure 3.</strong> How various single-precision floating-point values
are converted to two integers for encoding</p>
</div>
<p>To understand the solution used in git db, it helps to understand how
floating point formats work.  The <a class="reference external" href="http://en.wikipedia.org/wiki/Ieee_float#Basic_formats">Wikipedia IEEE Float Page</a> is at
the time of writing quite useful and informative.</p>
<p>The idea with floating point is to reduce the problem of working with
fractional values, to working with integers.  Much as how when dealing
with long multiplication on paper, you ignore the decimal points and
then at the end put the point at the correct point.  Floating poiNt
arithmetic is just like this, but in binary instead of base 10.</p>
<p>So, the floating point number is essentially two (binary) numbers: a
<em>mantissa</em> and an <em>exponent</em>.  When you do something like a
multiply, you multiply the two mantissas, leaving a much larger
mantissa, add the exponents together and then &quot;normalize&quot; the result
so that the mantissa is in range of the amount of space you have for
it.</p>
<p>The essence of the floating point encoding is to take these two
numbers from the float, adjust them so that they are in terms of <em>N</em> ×
2<sup>*M*</sup>, and then store those.  In terms of converting from
IEEE 754 formats, this means subtracting a <em>bias</em> for the exponent,
and usually putting a 1 at the front of the mantissa.  The reason for
this is that the only value which does not start with a 1 is 0, which
is considered a special case.</p>
<p>Once you have expressed the number as <em>N</em> × 2<sup>*M*</sup>, it can
then be <em>reduced</em> - while <em>N</em> is even, you can halve
it and subtract one from <em>M</em>.  Because of this, some floats
end up encoding to a sequence smaller than the native float, but this
is unusual.</p>
<p>There are some special cases required for full representation of IEEE
floats, and these are described below.</p>
<!-- list-table: Representation of IEEE specials
:widths: 33 33 33
:header-rows: 1

* - case
  - *N*
  - *M*
* - "normal" number
  - Mantissa, with implied 1 added (if required)
  - Exponent, with bias subtracted
* - "subnormal" numbers
  - Mantissa, no 1 added (it doesn't exist)
  - Exponent, with bias subtracted
* - zero (0)
  - 0
  - 0
* - infinity (∞)
  - 0
  - 1 for +∞ and -1 for -∞
* - Quiet NaN
  - 0
  - 2
* - Signalling NaN
  - 0
  - Unsigned mantissa value + 3 -->
<p>In C, the conversion in and out is a matter of including the relevant
floating point header file, and extracting the bits.  This can all be
done using fast integer math, without divisions, and without losing
any precision.</p>
<p>In languages where such low-level manipulation of machine words is not
available, conversion in is two IEEE math operations, and conversion
out is three or more, such as:</p>
<pre class="literal-block">
scale = int(log(num) / log(2));
mantissa = int(num * 2**(MANTISSA_BITS - scale));
exp = scale - MANTISSA_BITS;
while ( int( (mantissa+1)/2 ) == mantissa/2 ) {
    mantissa = mantissa / 2;
    exp++;
}
_return(mantissa, exp);
</pre>
<p><tt class="docutils literal">MANTISSA_BITS</tt> would be 53 for binary64 (double precision)
systems, 23 for binary32 (single precision) and so on.  You may lose a
few bits of precision with this approach, and it is slow - but it can
be done.  There is an obvious optimization, too - the final loop can
be done with higher powers of 2, to reduce the two integers with fewer
loop iterations.  0, the infinities and nan's will also require
special case branches.</p>
<p>However complicated emitting one of these pairs is from a high level
language, ignoring the exception cases which result in <tt class="docutils literal">mantissa ==
0</tt>, reconstructing is very simple:</p>
<pre class="literal-block">
num = mantissa * (2 ** exp)
</pre>
<p>Note, the difficulties around this format is not really down to the
storage format; <em>N</em> × 2<sup>*M*</sup> is very simple and it is the
IEEE 754 format which is (justifiably) more complicated.</p>
<p>This simplicity/space trade-off is quantified in the below table:</p>
<table border="1" class="docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Name</th>
<th class="head">Common Name</th>
<th class="head">Native Size</th>
<th class="head">Encoded Size</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>binary16</td>
<td>half precision</td>
<td>2 bytes</td>
<td>2-3 bytes</td>
</tr>
<tr><td>binary32</td>
<td>single precision</td>
<td>4 bytes</td>
<td>2-6 bytes</td>
</tr>
<tr><td>binary64</td>
<td>double precision</td>
<td>8 bytes</td>
<td>2-11 bytes</td>
</tr>
<tr><td>8087-80</td>
<td>&quot;extended precision&quot;</td>
<td>10 bytes</td>
<td>2-12 bytes</td>
</tr>
<tr><td>binary128</td>
<td>quadruple precision</td>
<td>16 bytes</td>
<td>2-19 bytes</td>
</tr>
</tbody>
</table>
<p>Of course, only numbers which are &quot;round numbers&quot; in binary, like 192,
0.125 and 16777216 will get the most compact end of the size range
regardless of the native float format.</p>
</div>
<div class="section" id="decimal-representation">
<h1>Decimal Representation</h1>
<p>The decimal representation is there primarily because most databases
support a fixed-precision type, eg NUMERIC(9,2), and it should be
possible to support this precisely when this is requested in the
schema.</p>
<p>There is another use for decimal representation.  For example, the
number 0.2 (base 10) will be a recurring number expressed in binary
(0.001100110011...) and cannot be converted exactly to an integer
times a power of two.</p>
<div class="figure align-right" style="width: 25em">
<img alt="diagram showing the value 0.2 encoded in various styles" src="../files/decimaletc.png" style="width: 20em;" />
<p class="caption"><strong>Figure 4.</strong> How the various encodings store &quot;0.2&quot;; the 'float'
encoding assumes a binary64 source platform.  You can see the
reciprocal repeating itself through the digits, and the effect of
ieee 754 rounding rules on the final bit.</p>
</div>
<p>205 × 2<sup>-10</sup>, the closest representation available in
binary16, is 0.200 - not bad for 16 bits.  In float format, at the
same precision, you're looking at 3 bytes of storage space.  Unpacked
on a double precision platform, it will render as 0.2001953125.</p>
<p>3355443 × 2<sup>-24</sup>, for single precision, encodes to
5 bytes.  In double precision, which most machines will use (and gcc
can emulate on platforms which don't support it natively), you end up
with 3602879701896397 × 2<sup>-54</sup>, taking a whopping 9
bytes to encode.</p>
<p>For the die-hard 8087 80-bit format,
3689348814741910323 × 2<sup>-64</sup>, encoding in 10 bytes,
is actually the same size as the original float.  If you happened to
find yourself on some machine with the ieee128 format,
1038459371706965525706099265844019 × 2<sup>-112</sup>
encodes to an 18 byte sequence.</p>
<p>This highlights a point; much extra space is used for precision which
isn't really there, and in fact it is never precisely the same as the
simple string &quot;0.2&quot;.  Therefore, some implementations might choose to
try converting floating point values to decimal before they are
stored, and if the mantissa in decimal (rendered to the IEEE decimal
rules) is significantly smaller than the maximum float mantissa value,
then to store the value using the decimal encoding instead.</p>
<p>If an implementation does decide that &quot;0.2&quot; is a decimal, and not a
floating point value, it can store it as
2 × 10<sup>-1</sup>, which is only 2 bytes and fully precise
on all floating point platforms.</p>
</div>
<div class="section" id="rational-representation">
<h1>Rational Representation</h1>
<p>This format can store the set of rational numbers, and is included as
alongside the other two-integer formats it is trivial to decode and
infinitely expands the precisely representable numeric values.  While
<em>N</em> × 2<sup>M</sup> is an infinite set, there is another
infinite set of other precise numeric values as well.  Sure, there are
useful irrationals one could precisely encode like (₁₂√2)⁵ - a major
3rd - but <a class="reference external" href="http://www.ehow.com/how_2187418_tune-guitar-using-harmonics.html">4/3 is usually close enough to a third to tune a guitar</a>.</p>
<p>In this format the numerator is a signed variable length integer, and
the denominator is an unsigned one.  The denominator may not be zero.</p>
</div>
<div class="section" id="encoding-strings">
<h1>Encoding strings</h1>
<div class="figure align-right" style="width: 20em">
<img alt="diagram showing the string Māori encoded in various styles" src="../files/text.png" style="width: 17em;" />
<p class="caption"><strong>Figure 5.</strong> how the NFD representation of the string &quot;Māori&quot; is
encoded.  UTF-8 multi-byte sequences are similar to the <tt class="docutils literal">varuint</tt>
encoding, but use the top bit to indicate that the byte is a part
of a multi-byte sequence.</p>
</div>
<p>These are encoded with a variable length unsigned integer, followed by
a quantity of bytes.  Neither the encoding nor the column format has
any knowledge of UTF-8; however there are two <a class="reference external" href="./meta.html#types">standard types</a> which
have different functions depending on whether the content is a byte
string, or text.</p>
</div>
</div>

  </div>

  <div id="footer" style="clear: both">
   <hr />
   <a href="../copyright.html">copyright info</a> |
   <a href="../map.html">site map</a>
  </div>

</body>
</html>
